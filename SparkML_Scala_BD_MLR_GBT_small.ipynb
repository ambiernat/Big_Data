{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "systematic-identity",
   "metadata": {},
   "source": [
    "#Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stable-technical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\ncustomSchema: org.apache.spark.sql.types.StructType = StructType(StructField(asin,StringType,true), StructField(overall,DoubleType,true), StructField(reviewtext,StringType,true), StructField(category,StringType,true))\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val customSchema = StructType(Array(\n",
    "  StructField(\"asin\", StringType, true),\n",
    "  StructField(\"overall\", DoubleType, true),\n",
    "  StructField(\"reviewtext\", StringType, true),\n",
    "  StructField(\"category\", StringType, true))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "selective-research",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [asin: string, overall: double ... 2 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.format(\"csv\")\n",
    "  .option(\"sep\", \",\")\n",
    "  .option(\"inferSchema\", \"false\")\n",
    "  .schema(customSchema)\n",
    "  .option(\"header\", \"false\")\n",
    "  .load(\"hdfs:///user/maria_dev/for_nlp_models.csv\")\n",
    "  .na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "weighted-performer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.registerTempTable(\"ratings1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "golden-breath",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall\tcount_ratings\n1.0\t136686\n2.0\t129119\n3.0\t296208\n4.0\t684735\n5.0\t1864279\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql\n",
    "select overall, count(*) as count_ratings from ratings1 group by overall order by overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "verbal-spyware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{coalesce, lit, typedLit}\ntranslationMap: scala.collection.immutable.Map[Int,Int] = Map(5 -> 2, 1 -> 0, 2 -> 0, 3 -> 1, 4 -> 2)\ntranslationMapCol: org.apache.spark.sql.Column = keys: [5,1,2,3,4], values: [2,0,0,1,2]\nDF: org.apache.spark.sql.DataFrame = [asin: string, overall: double ... 3 more fields]\nroot\n |-- asin: string (nullable = true)\n |-- overall: double (nullable = true)\n |-- reviewtext: string (nullable = true)\n |-- category: string (nullable = true)\n |-- label: double (nullable = true)\n\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{coalesce, lit, typedLit}\r\n",
    "\r\n",
    "// create a mapping\r\n",
    "val translationMap = Map(\r\n",
    "  1 -> 0,\r\n",
    "  2 -> 0,\r\n",
    "  3 -> 1, \r\n",
    "  4 -> 2,\r\n",
    "  5 -> 2\r\n",
    ")\r\n",
    "\r\n",
    "// convert a map to column\r\n",
    "val translationMapCol = typedLit(translationMap)\r\n",
    "\r\n",
    "// add translation\r\n",
    "val DF = data.withColumn(\"label\", coalesce(translationMapCol($\"overall\"), lit(\"\")).cast(DoubleType))\r\n",
    "\r\n",
    "DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "built-merit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.registerTempTable(\"ratings2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "complete-press",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\tcount_ratings\n0.0\t265805\n1.0\t296208\n2.0\t2549014\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql\n",
    "select label, count(*) as count_ratings from ratings2 group by label order by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "classical-month",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res897: Long = 3111027\nres898: Array[String] = Array(asin, overall, reviewtext, category, label)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "DF.count\n",
    "DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "plain-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    " // subsample a smaller dataset and filter only needed columns\n",
    "val sampSize = 500000\n",
    "val frac = sampSize.toDouble/data.count\n",
    "val sampledData = DF.sample(withReplacement = false, fraction = frac, seed = 42) //.select(\"asin\", \")\n",
    "sampledData.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adult-authentication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratingsDF: org.apache.spark.sql.DataFrame = [asin: string, overall: double ... 3 more fields]\nroot\n |-- asin: string (nullable = true)\n |-- overall: double (nullable = true)\n |-- reviewtext: string (nullable = true)\n |-- category: string (nullable = true)\n |-- label: double (nullable = true)\n\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// assign a table that will be modelled in subsequent parts\n",
    "val ratingsDF = sampSize // DF or sampledData\n",
    "\n",
    "// = sampledData.toDF()//.withColumnRenamed(\"overall\", \"label\")\n",
    "ratingsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "industrial-polls",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res909: Long = 3111027\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsDF.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dressed-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "// check for duplicate reviews\n",
    "// val uniqueCheck = ratingsDF.groupBy(\"asin\", \"reviewerid\", \"overall\").count().orderBy(desc(\"count\"))\n",
    "// uniqueCheck.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aggressive-hostel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DF: org.apache.spark.sql.DataFrame = [asin: string, overall: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// drop duplicates\n",
    "\n",
    "\n",
    "val DF = ratingsDF\n",
    "// val DF = ratingsDF.dropDuplicates(Array(\"asin\",\"reviewerid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "recognized-seeker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DF2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [asin: string, overall: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val DF2 = DF.filter($\"reviewtext\".isNotNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "choice-poland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vocabSize: Long = 1020666\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vocabSize = DF2.withColumn(\"reviewtext\", lower($\"reviewtext\")).withColumn(\"reviewtext\", split(col(\"reviewtext\"), \" \").cast(\"array<string>\")).select($\"asin\", explode($\"reviewtext\").alias(\"exploded\")).groupBy(\"exploded\").count().count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "brave-remainder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer, StopWordsRemover, CountVectorizer, HashingTF, IDF}\nimport scala.math.pow\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\nregextokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_4d346c993a00\nsw_remover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_d6d4ee83814a\nhashtf: org.apache.spark.ml.feature.HashingTF = hashingTF_b953c3a1f17d\nidf: org.apache.spark.ml.feature.IDF = idf_26ea91ca7d7f\nindexer: org.apache.spark.ml.feature.StringIndexer = strIdx_8eac59d0d48d\nvassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_3c15388df547\npreprocessing_stages: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.param.shared.HasOutputCol with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.param.shared.HasOutputCol with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.param.shared.HasOutputCol with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.param.shared.HasOutputCol with org.apache.spark.ml.util.DefaultParamsWritable}}}] = Array(regexTok_4d346c993a00, stopWords_d6d4ee83814a, has...preprocessing_pipeline: org.apache.spark.ml.Pipeline = pipeline_f5e5cde75c15\ntrainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [asin: string, overall: double ... 3 more fields]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [asin: string, overall: double ... 3 more fields]\nlabelColumn: String = label\nStarting Multinomial Logistic Regression\nlambda_par: Double = 0.02\nalpha_par: Double = 0.3\nen_mlr: org.apache.spark.ml.classification.LogisticRegression = logreg_386cc6dc2c4a\nmodel_stages: Array[org.apache.spark.ml.Estimator[_ >: org.apache.spark.ml.classification.LogisticRegressionModel with org.apache.spark.ml.PipelineModel <: org.apache.spark.ml.Model[_ >: org.apache.spark.ml.classification.LogisticRegressionModel with org.apache.spark.ml.PipelineModel <: org.apache.spark.ml.Transformer with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.util.MLWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.Estimator[_ >: org.apache.spark.ml.classification.LogisticRegressionModel with org.apache.spark.ml.PipelineModel <: org.apache.spark.ml.Transformer with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.util.MLWritable{def copy(extra: org.apache.spark.ml.param.ParamM...model_pipeline: org.apache.spark.ml.Pipeline = pipeline_5e09bbcf2d07\nparamGrid: Array[org.apache.spark.ml.param.ParamMap] =\nArray({\n\tlogreg_386cc6dc2c4a-elasticNetParam: 0.1,\n\tlogreg_386cc6dc2c4a-regParam: 0.01\n}, {\n\tlogreg_386cc6dc2c4a-elasticNetParam: 0.2,\n\tlogreg_386cc6dc2c4a-regParam: 0.01\n}, {\n\tlogreg_386cc6dc2c4a-elasticNetParam: 0.4,\n\tlogreg_386cc6dc2c4a-regParam: 0.01\n}, {\n\tlogreg_386cc6dc2c4a-elasticNetParam: 0.1,\n\tlogreg_386cc6dc2c4a-regParam: 0.05\n}, {\n\tlogreg_386cc6dc2c4a-elasticNetParam: 0.2,\n\tlogreg_386cc6dc2c4a-regParam: 0.05\n}, {\n\tlogreg_386cc6dc2c4a-elasticNetParam: 0.4,\n\tlogreg_386cc6dc2c4a-regParam: 0.05\n}, {\n\tlogreg_386cc6dc2c4a-elasticNetParam: 0.1,\n\tlogreg_386cc6dc2c4a-regParam: 0.1\n}, {\n\tlogreg_386cc6dc2c4a-elasticNetParam: 0.2,\n\tlogreg_386cc6dc2c4a-regParam: 0.1\n}, {\n\tlogreg_386cc6dc2c4a-elasticNetParam: 0.4,\n\tlogreg_386cc6dc2c4a-...evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_90f8c1da3c09\ncrossval: org.apache.spark.ml.tuning.CrossValidator = cv_d2afceaa8047\ncvModelMLR: org.apache.spark.ml.tuning.CrossValidatorModel = cv_d2afceaa8047\n<console>:3: error: ';' expected but 'val' found.\n%val cvModelMLR = PipelineModel.load(\"hdfs:///user/maria_dev/spark-mlr-cv-best\")\n ^\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer, StopWordsRemover, CountVectorizer, HashingTF, IDF}\n",
    "import scala.math.pow\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\n",
    "\n",
    "\n",
    "//// TEXT COLUMN PREPROCESSING\n",
    "//We define the RegexTokenizer() to convert reviews from plain text form to words\n",
    "val regextokenizer = new RegexTokenizer().setGaps(false)\n",
    "    .setPattern(\"\\\\w+(?:'\\\\w+)?|[^\\\\w\\\\s]\")\n",
    "    .setInputCol(\"reviewtext\")\n",
    "    .setOutputCol(\"reviewwords\")\n",
    "    \n",
    "    \n",
    "//We define remove the stop words from the reviewwords\n",
    "val sw_remover = new StopWordsRemover()\n",
    "    .setCaseSensitive(false)\n",
    "    .setInputCol(\"reviewwords\")\n",
    "    .setOutputCol(\"words_filtered\")\n",
    "    \n",
    "    \n",
    "//  Hashing TF to vectorize features\n",
    "val hashtf = new HashingTF()\n",
    "            .setInputCol(\"words_filtered\")\n",
    "            .setOutputCol(\"tf\")\n",
    "            .setNumFeatures(pow(2,12).toInt)\n",
    "            \n",
    "            \n",
    "// Lowers the terms of documents that are very common\n",
    "val idf = new IDF()\n",
    "        .setInputCol(\"tf\")\n",
    "        .setOutputCol(\"tfidf\")\n",
    "\n",
    "// CATEGORY COLUMN I.E. CATEGORICAL COLUMN PREPROCESSING\n",
    "// convert categorical column 'category' to labels; no rule for assigning the labels is used, the transformation is done in a random manner\n",
    "val indexer = new StringIndexer()\n",
    "        .setInputCol(\"category\")\n",
    "        .setOutputCol(\"categorylabel\")\n",
    "        .setHandleInvalid(\"keep\") //telling StringIndexer how to treat unseen labels - 'skip' will remove the rows entirely; 'keep' will assign the same index to all unseen\n",
    "\n",
    "\n",
    "// assemble two features columns into a 'features' column\n",
    "val vassembler = new VectorAssembler()\n",
    "        .setInputCols(Array(\"tfidf\", \"categorylabel\"))\n",
    "        .setOutputCol(\"features\")\n",
    "\n",
    "\n",
    "// Set the pipeline    \n",
    "val preprocessing_stages = Array(regextokenizer, sw_remover, hashtf, idf, indexer, vassembler)\n",
    "val preprocessing_pipeline = new Pipeline()\n",
    "                    .setStages(preprocessing_stages)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    \n",
    "// Split into training and test datasets and, just to be safe, define the target column                  \n",
    "val Array(trainingData, testData) = DF2.randomSplit(Array(0.8, 0.2), seed=42)\n",
    "val labelColumn = \"label\"\n",
    "\n",
    "\n",
    "println(\"Starting Multinomial Logistic Regression\")\n",
    "// Initialise hyperparameters that will be later tuned and define the multinomial logistic regression model, including lambda parameter corresponding to regularization\n",
    "val lambda_par = 0.02\n",
    "val alpha_par = 0.3\n",
    "val en_mlr = new LogisticRegression()\n",
    "        .setLabelCol(labelColumn)\n",
    "        .setFeaturesCol(\"features\")\n",
    "        .setRegParam(lambda_par)\n",
    "        .setMaxIter(100)\n",
    "        .setElasticNetParam(alpha_par)\n",
    "        .setFamily(\"multinomial\")\n",
    "        \n",
    "        \n",
    "\n",
    "// Define the full model pipeline - preprocessing + model       \n",
    "val model_stages = Array(preprocessing_pipeline, en_mlr) \n",
    "val model_pipeline = new Pipeline()\n",
    "                    .setStages(model_stages)\n",
    "                    \n",
    "                    \n",
    "// Define the param grid used for tuning the hyperparameters                    \n",
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(en_mlr.regParam, Array(0.01, 0.05, 0.1))\n",
    "  .addGrid(en_mlr.elasticNetParam, Array(0.1, 0.2, 0.4))\n",
    "  .build()\n",
    "  \n",
    "  \n",
    "// Define the evaluator used for evaluating the model\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "                .setLabelCol(labelColumn)\n",
    "\n",
    "\n",
    "// We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "// This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "// A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "// metricName() param for metric name in evaluation (supports \"f1\" (default), \"weightedPrecision\", \"weightedRecall\", \"accuracy\")\n",
    "// Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\n",
    "// is F1 score\n",
    "\n",
    "val crossval = new CrossValidator()\n",
    "  .setEstimator(model_pipeline)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(3)  // Use 3+ in practice\n",
    "  \n",
    "// Run cross-validation, and choose the best set of parameters.\n",
    "val cvModelMLR = crossval.fit(trainingData)\n",
    "\n",
    "// Now we can optionally save the fitted pipeline to disk\n",
    "// +cvModelMLR.write.overwrite().save(\"hdfs:///user/maria_dev/spark-mlr-cv-best\")\n",
    "\n",
    "// And load it back in during production\n",
    "val cvModelMLR = PipelineModel.load(\"hdfs:///user/maria_dev/spark-mlr-cv-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "demanding-swift",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\nresults: org.apache.spark.sql.DataFrame = [asin: string, reviewtext: string ... 4 more fields]\npredictionAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[39528] at rdd at <console>:199\nmetrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@ce39930\nConfusion matrix:\n14125.0  2593.0  33628.0   \n3363.0   6536.0  46984.0   \n2692.0   3281.0  491108.0  \naccuracy: Double = 0.846865019609141\nSummary Statistics\nAccuracy = 0.846865019609141\nWeighted precision: 0.8144684381243038\nWeighted recall: 0.8468650196091411\nWeighted F1 score: 0.8070515918639329\nWeighted false positive rate: 0.6202997707641787\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "\n",
    "\n",
    "// Make predictions on test documents. cvModel uses the best model found (multinomial logistic regression model).\n",
    "val results = cvModelMLR.transform(testData)\n",
    "                     .select(\"asin\", \"reviewtext\", \"category\", \"label\", \"probability\", \"prediction\")\n",
    "                     \n",
    "// Compute raw scores on the test set\n",
    "val predictionAndLabels = results\n",
    "        .select($\"prediction\",$\"label\")\n",
    "        .as[(Double, Double)]\n",
    "        .rdd\n",
    "        \n",
    "// EVALUATE THE MODEL\n",
    "        \n",
    "// Instantiate metrics object\n",
    "val metrics = new MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "// Confusion matrix\n",
    "println(\"Confusion matrix:\")\n",
    "println(metrics.confusionMatrix)\n",
    "\n",
    "// Overall Statistics\n",
    "val accuracy = metrics.accuracy\n",
    "println(\"Summary Statistics\")\n",
    "println(s\"Accuracy = $accuracy\")\n",
    "\n",
    "\n",
    "// Weighted stats\n",
    "println(s\"Weighted precision: ${metrics.weightedPrecision}\")\n",
    "println(s\"Weighted recall: ${metrics.weightedRecall}\")\n",
    "println(s\"Weighted F1 score: ${metrics.weightedFMeasure}\")\n",
    "println(s\"Weighted false positive rate: ${metrics.weightedFalsePositiveRate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-definition",
   "metadata": {},
   "source": [
    "#Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ordered-finland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\ncustomSchema: org.apache.spark.sql.types.StructType = StructType(StructField(asin,StringType,true), StructField(overall,DoubleType,true), StructField(reviewtext,StringType,true), StructField(category,StringType,true))\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val customSchema = StructType(Array(\n",
    "  StructField(\"asin\", StringType, true),\n",
    "  StructField(\"overall\", DoubleType, true),\n",
    "  StructField(\"reviewtext\", StringType, true),\n",
    "  StructField(\"category\", StringType, true))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "chronic-theater",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [asin: string, overall: double ... 2 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.format(\"csv\")\n",
    "  .option(\"sep\", \",\")\n",
    "  .option(\"inferSchema\", \"false\")\n",
    "  .schema(customSchema)\n",
    "  .option(\"header\", \"false\")\n",
    "  .load(\"hdfs:///user/maria_dev/for_nlp_models.csv\")\n",
    "  .na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "referenced-airfare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.registerTempTable(\"ratings3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "drawn-government",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall\tcount_ratings\n1.0\t136686\n2.0\t129119\n3.0\t296208\n4.0\t684735\n5.0\t1864279\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql\n",
    "select overall, count(*) as count_ratings from ratings3 group by overall order by overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "velvet-connectivity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{coalesce, lit, typedLit}\ntranslationMap: scala.collection.immutable.Map[Int,Int] = Map(5 -> 1, 1 -> 0, 2 -> 0, 3 -> 0, 4 -> 1)\ntranslationMapCol: org.apache.spark.sql.Column = keys: [5,1,2,3,4], values: [1,0,0,0,1]\nDF: org.apache.spark.sql.DataFrame = [asin: string, overall: double ... 3 more fields]\nroot\n |-- asin: string (nullable = true)\n |-- overall: double (nullable = true)\n |-- reviewtext: string (nullable = true)\n |-- category: string (nullable = true)\n |-- label: double (nullable = true)\n\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{coalesce, lit, typedLit}\r\n",
    "\r\n",
    "// create a mapping\r\n",
    "val translationMap = Map(\r\n",
    "  1 -> 0,\r\n",
    "  2 -> 0,\r\n",
    "  3 -> 0, \r\n",
    "  4 -> 1,\r\n",
    "  5 -> 1\r\n",
    ")\r\n",
    "\r\n",
    "// convert a map to column\r\n",
    "val translationMapCol = typedLit(translationMap)\r\n",
    "\r\n",
    "// add translation\r\n",
    "val DF = data.withColumn(\"label\", coalesce(translationMapCol($\"overall\"), lit(\"\")).cast(DoubleType))\r\n",
    "\r\n",
    "DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "oriental-greenhouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.registerTempTable(\"ratings4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "caroline-former",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\tcount_ratings\n0.0\t562013\n1.0\t2549014\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql\n",
    "select label, count(*) as count_ratings from ratings4 group by label order by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "informational-commission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1011: Long = 3111027\nres1012: Array[String] = Array(asin, overall, reviewtext, category, label)\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.count\n",
    "DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "functioning-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    " // subsample a smaller dataset and filter only needed columns\n",
    " val sampSize = 500000\n",
    " val frac = sampSize.toDouble/data.count\n",
    " val sampledData = DF.sample(withReplacement = false, fraction = frac, seed = 42) //.select(\"asin\", \")\n",
    " sampledData.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "temporal-house",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratingsDF: org.apache.spark.sql.DataFrame = [asin: string, overall: double ... 3 more fields]\nroot\n |-- asin: string (nullable = true)\n |-- overall: double (nullable = true)\n |-- reviewtext: string (nullable = true)\n |-- category: string (nullable = true)\n |-- label: double (nullable = true)\n\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// assign a table that will be modelled in subsequent parts\n",
    "val ratingsDF = sampledData // DF or sampledData\n",
    "\n",
    "// = sampledData.toDF()//.withColumnRenamed(\"overall\", \"label\")\n",
    "ratingsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "sustained-broadcast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1023: Long = 3111027\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsDF.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "pressing-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "// check for duplicate reviews\n",
    "//val uniqueCheck = ratingsDF.groupBy(\"asin\", \"reviewerid\", \"overall\").count().orderBy(desc(\"count\"))\n",
    "//uniqueCheck.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "otherwise-application",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DF: org.apache.spark.sql.DataFrame = [asin: string, overall: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// drop duplicates\n",
    "\n",
    "val DF = ratingsDF\n",
    "// val DF = ratingsDF.dropDuplicates(Array(\"asin\",\"reviewerid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "confirmed-tanzania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DF2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [asin: string, overall: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val DF2 = DF.filter($\"reviewtext\".isNotNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "isolated-mailman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vocabSize: Long = 1020666\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vocabSize = DF2.withColumn(\"reviewtext\", lower($\"reviewtext\")).withColumn(\"reviewtext\", split(col(\"reviewtext\"), \" \").cast(\"array<string>\")).select($\"asin\", explode($\"reviewtext\").alias(\"exploded\")).groupBy(\"exploded\").count().count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "desirable-scanner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer, StopWordsRemover, CountVectorizer, HashingTF, IDF}\nimport scala.math.pow\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\nimport org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\nregextokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_ec5d0aff7616\nsw_remover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_3ee15eaca8ce\nhashtf: org.apache.spark.ml.feature.HashingTF = hashingTF_f1ab31af8bc2\nidf: org.apache.spark.ml.feature.IDF = idf_e1a1b6769621\nindexer: org.apache.spark.ml.feature.StringIndexer = strIdx_0e663b8a9987\nvassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_c4e0c913fca6\npreprocessing_stages: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.param.shared.HasOutputCol with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.param.shared.HasOutputCol with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.param.shared.HasOutputCol with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.param.shared.HasOutputCol with org.apache.spark.ml.util.DefaultParamsWritable}}}] = Array(regexTok_ec5d0aff7616, stopWords_3ee15eaca8ce, has...preprocessing_pipeline: org.apache.spark.ml.Pipeline = pipeline_5a5c2c4a5363\ntrainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [asin: string, overall: double ... 3 more fields]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [asin: string, overall: double ... 3 more fields]\nlabelColumn: String = label\nStarting GBTree Classification\ngbt: org.apache.spark.ml.classification.GBTClassifier = gbtc_a60213f1fd28\nmodel_stages: Array[org.apache.spark.ml.Estimator[_ >: org.apache.spark.ml.classification.GBTClassificationModel with org.apache.spark.ml.PipelineModel <: org.apache.spark.ml.Model[_ >: org.apache.spark.ml.classification.GBTClassificationModel with org.apache.spark.ml.PipelineModel <: org.apache.spark.ml.Transformer with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.util.MLWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.Estimator[_ >: org.apache.spark.ml.classification.GBTClassificationModel with org.apache.spark.ml.PipelineModel <: org.apache.spark.ml.Transformer with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.util.MLWritable{def copy(extra: org.apache.spark.ml.param.ParamMap)...model_pipeline: org.apache.spark.ml.Pipeline = pipeline_2f616e5468c3\norg.apache.spark.SparkException: Job 9103 cancelled part of cancelled job group zeppelin-20210601-100018_1360155050\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1539)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:811)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1789)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)\n  at org.apache.spark.ml.tree.impl.RandomForest$.findSplitsBySorting(RandomForest.scala:928)\n  at org.apache.spark.ml.tree.impl.RandomForest$.findSplits(RandomForest.scala:906)\n  at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:118)\n  at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:127)\n  at org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:297)\n  at org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:55)\n  at org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:179)\n  at org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n  at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n  at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n  at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:153)\n  at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:149)\n  at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n  at scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:44)\n  at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:37)\n  at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:149)\n  ... 81 elided\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer, StopWordsRemover, CountVectorizer, HashingTF, IDF}\n",
    "import scala.math.pow\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\n",
    "\n",
    "\n",
    "//// TEXT COLUMN PREPROCESSING\n",
    "//We define the RegexTokenizer() to convert reviews from plain text form to words\n",
    "val regextokenizer = new RegexTokenizer().setGaps(false)\n",
    "    .setPattern(\"\\\\w+(?:'\\\\w+)?|[^\\\\w\\\\s]\")\n",
    "    .setInputCol(\"reviewtext\")\n",
    "    .setOutputCol(\"reviewwords\")\n",
    "    \n",
    "    \n",
    "//We define remove the stop words from the reviewwords\n",
    "val sw_remover = new StopWordsRemover()\n",
    "    .setCaseSensitive(false)\n",
    "    .setInputCol(\"reviewwords\")\n",
    "    .setOutputCol(\"words_filtered\")\n",
    "    \n",
    "    \n",
    "//  Hashing TF to vectorize features\n",
    "val hashtf = new HashingTF()\n",
    "            .setInputCol(\"words_filtered\")\n",
    "            .setOutputCol(\"tf\")\n",
    "            .setNumFeatures(pow(2,12).toInt)\n",
    "            \n",
    "            \n",
    "// Lowers the terms of documents that are very common\n",
    "val idf = new IDF()\n",
    "        .setInputCol(\"tf\")\n",
    "        .setOutputCol(\"tfidf\")\n",
    "\n",
    "// CATEGORY COLUMN I.E. CATEGORICAL COLUMN PREPROCESSING\n",
    "// convert categorical column 'category' to labels; no rule for assigning the labels is used, the transformation is done in a random manner\n",
    "val indexer = new StringIndexer()\n",
    "        .setInputCol(\"category\")\n",
    "        .setOutputCol(\"categorylabel\")\n",
    "        .setHandleInvalid(\"keep\") //telling StringIndexer how to treat unseen labels - 'skip' will remove the rows entirely; 'keep' will assign the same index to all unseen\n",
    "\n",
    "\n",
    "// assemble two features columns into a 'features' column\n",
    "val vassembler = new VectorAssembler()\n",
    "        .setInputCols(Array(\"tfidf\", \"categorylabel\"))\n",
    "        .setOutputCol(\"features\")\n",
    "\n",
    "\n",
    "// Set the pipeline    \n",
    "val preprocessing_stages = Array(regextokenizer, sw_remover, hashtf, idf, indexer, vassembler)\n",
    "val preprocessing_pipeline = new Pipeline()\n",
    "                    .setStages(preprocessing_stages)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    \n",
    "// Split into training and test datasets and, just to be safe, define the target column                  \n",
    "val Array(trainingData, testData) = DF2.randomSplit(Array(0.8, 0.2), seed=42)\n",
    "val labelColumn = \"label\"\n",
    "\n",
    "\n",
    "println(\"Starting GBTree Classification\")\n",
    "// For the classification we'll use the Gradient-boosted tree estimator\n",
    "// using some default and set hyper-parameters; no crossvalidation is done as this was very computationally expensive and tool a lot of time\n",
    "val gbt = new GBTClassifier()\n",
    "    .setLabelCol(labelColumn)\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setMaxIter(10)\n",
    "    .setMaxBins(10000)\n",
    "    \n",
    "    \n",
    "// Define the full model pipeline - preprocessing + model       \n",
    "val model_stages = Array(preprocessing_pipeline, gbt) \n",
    "val model_pipeline = new Pipeline()\n",
    "                    .setStages(model_stages)\n",
    "     \n",
    "               \n",
    "//We fit our DataFrame into the pipeline to generate a model\n",
    "val model = model_pipeline.fit(trainingData)\n",
    "\n",
    "\n",
    "//We'll make predictions using the model and the test data\n",
    "//val predictions = model.transform(testData)\n",
    "\n",
    "\n",
    "// Define the evaluator used for evaluating the model\n",
    "// val evaluator = new BinaryClassificationEvaluator()\n",
    "//               .setLabelCol(labelColumn)\n",
    "                \n",
    "\n",
    "// Now we can optionally save the fitted pipeline to disk\n",
    "model.write.overwrite().save(\"hdfs:///user/maria_dev/spark-gbt-first\")\n",
    "\n",
    "// And load it back in during production\n",
    "val model = PipelineModel.load(\"hdfs:///user/maria_dev/spark-gbt-first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "small-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.evaluation.{MulticlassMetrics, BinaryClassificationMetrics}\n",
    "\n",
    "\n",
    "// We'll calculate results using the model and the test data\n",
    "val results = model.transform(testData)\n",
    "                     .select(\"asin\", \"reviewtext\", \"category\", \"label\", \"probability\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fifteen-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Compute raw scores on the test set\n",
    "val predictionAndLabels = results\n",
    "        .select($\"prediction\",$\"label\")\n",
    "        .as[(Double, Double)]\n",
    "        .rdd\n",
    "        \n",
    "// EVALUATE THE MODEL\n",
    "        \n",
    "// Instantiate metrics object\n",
    "val metrics = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "\n",
    "// Confusion matrix\n",
    "//println(\"Confusion matrix:\")\n",
    "//println(metrics.confusionMatrix)\n",
    "\n",
    "// Overall Statistics\n",
    "//val accuracy = metrics.accuracy\n",
    "//println(\"Summary Statistics\")\n",
    "//println(s\"Accuracy = $accuracy\")\n",
    "\n",
    "\n",
    "// Weighted stats\n",
    "//println(s\"Weighted precision: ${metrics.precisionByThreshold}\")\n",
    "//println(s\"Weighted recall: ${metrics.recallByThreshold}\")\n",
    "//println(s\"Weighted F1 score: ${metrics.fMeasureByThreshold}\")\n",
    "println(s\"Area under ROC: ${metrics.areaUnderROC}\")\n",
    "\n",
    "// Precision by threshold\n",
    "val precision = metrics.precisionByThreshold\n",
    "precision.foreach { case (t, p) =>\n",
    "  println(s\"Threshold: $t, Precision: $p\")\n",
    "}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
