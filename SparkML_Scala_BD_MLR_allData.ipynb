{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unlike-spice",
   "metadata": {},
   "source": [
    "# Multiple Logistic Regression - all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tight-orleans",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\ncustomSchema: org.apache.spark.sql.types.StructType = StructType(StructField(asin,StringType,true), StructField(overall,DoubleType,true), StructField(reviewtext,StringType,true), StructField(category,StringType,true))\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val customSchema = StructType(Array(\n",
    "  StructField(\"asin\", StringType, true),\n",
    "  StructField(\"overall\", DoubleType, true),\n",
    "  StructField(\"reviewtext\", StringType, true),\n",
    "  StructField(\"category\", StringType, true))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "orange-guinea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [asin: string, overall: double ... 2 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.format(\"csv\")\n",
    "  .option(\"sep\", \",\")\n",
    "  .option(\"inferSchema\", \"false\")\n",
    "  .schema(customSchema)\n",
    "  .option(\"header\", \"false\")\n",
    "  .load(\"hdfs:///user/maria_dev/for_nlp_models.csv\")\n",
    "  .na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sunrise-basket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.registerTempTable(\"ratings1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "established-sauce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall\tcount_ratings\n1.0\t136686\n2.0\t129119\n3.0\t296208\n4.0\t684735\n5.0\t1864279\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql\n",
    "select overall, count(*) as count_ratings from ratings1 group by overall order by overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "human-english",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{coalesce, lit, typedLit}\ntranslationMap: scala.collection.immutable.Map[Int,Int] = Map(5 -> 2, 1 -> 0, 2 -> 0, 3 -> 1, 4 -> 2)\ntranslationMapCol: org.apache.spark.sql.Column = keys: [5,1,2,3,4], values: [2,0,0,1,2]\nDF: org.apache.spark.sql.DataFrame = [asin: string, overall: double ... 3 more fields]\nroot\n |-- asin: string (nullable = true)\n |-- overall: double (nullable = true)\n |-- reviewtext: string (nullable = true)\n |-- category: string (nullable = true)\n |-- label: double (nullable = true)\n\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{coalesce, lit, typedLit}\r\n",
    "\r\n",
    "// create a mapping\r\n",
    "val translationMap = Map(\r\n",
    "  1 -> 0,\r\n",
    "  2 -> 0,\r\n",
    "  3 -> 1, \r\n",
    "  4 -> 2,\r\n",
    "  5 -> 2\r\n",
    ")\r\n",
    "\r\n",
    "// convert a map to column\r\n",
    "val translationMapCol = typedLit(translationMap)\r\n",
    "\r\n",
    "// add translation\r\n",
    "val DF = data.withColumn(\"label\", coalesce(translationMapCol($\"overall\"), lit(\"\")).cast(DoubleType))\r\n",
    "\r\n",
    "DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "surprised-voice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.registerTempTable(\"ratings2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "enclosed-growth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\tcount_ratings\n0.0\t265805\n1.0\t296208\n2.0\t2549014\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql\n",
    "select label, count(*) as count_ratings from ratings2 group by label order by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "closing-native",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res897: Long = 3111027\nres898: Array[String] = Array(asin, overall, reviewtext, category, label)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "DF.count\n",
    "DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "yellow-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    " //// subsample a smaller dataset and filter only needed columns\n",
    "// val sampSize = 20000\n",
    "// val frac = sampSize.toDouble/data.count\n",
    "// val sampledData = DF.sample(withReplacement = false, fraction = frac, seed = 42) //.select(\"asin\", \")\n",
    "// sampledData.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rapid-handy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratingsDF: org.apache.spark.sql.DataFrame = [asin: string, overall: double ... 3 more fields]\nroot\n |-- asin: string (nullable = true)\n |-- overall: double (nullable = true)\n |-- reviewtext: string (nullable = true)\n |-- category: string (nullable = true)\n |-- label: double (nullable = true)\n\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// assign a table that will be modelled in subsequent parts\n",
    "val ratingsDF = DF // DF or sampledData\n",
    "\n",
    "// = sampledData.toDF()//.withColumnRenamed(\"overall\", \"label\")\n",
    "ratingsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "residential-january",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res909: Long = 3111027\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsDF.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adult-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "// check for duplicate reviews\n",
    "// val uniqueCheck = ratingsDF.groupBy(\"asin\", \"reviewerid\", \"overall\").count().orderBy(desc(\"count\"))\n",
    "// uniqueCheck.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "positive-darwin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DF: org.apache.spark.sql.DataFrame = [asin: string, overall: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// drop duplicates\n",
    "\n",
    "\n",
    "val DF = ratingsDF\n",
    "// val DF = ratingsDF.dropDuplicates(Array(\"asin\",\"reviewerid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "nominated-beaver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DF2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [asin: string, overall: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val DF2 = DF.filter($\"reviewtext\".isNotNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bright-bench",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vocabSize: Long = 1020666\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vocabSize = DF2.withColumn(\"reviewtext\", lower($\"reviewtext\")).withColumn(\"reviewtext\", split(col(\"reviewtext\"), \" \").cast(\"array<string>\")).select($\"asin\", explode($\"reviewtext\").alias(\"exploded\")).groupBy(\"exploded\").count().count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "extensive-struggle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer, StopWordsRemover, CountVectorizer, HashingTF, IDF}\nimport scala.math.pow\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\nregextokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_4d346c993a00\nsw_remover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_d6d4ee83814a\nhashtf: org.apache.spark.ml.feature.HashingTF = hashingTF_b953c3a1f17d\nidf: org.apache.spark.ml.feature.IDF = idf_26ea91ca7d7f\nindexer: org.apache.spark.ml.feature.StringIndexer = strIdx_8eac59d0d48d\nvassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_3c15388df547\npreprocessing_stages: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.param.shared.HasOutputCol with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.param.shared.HasOutputCol with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.param.shared.HasOutputCol with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.param.shared.HasOutputCol with org.apache.spark.ml.util.DefaultParamsWritable}}}] = Array(regexTok_4d346c993a00, stopWords_d6d4ee83814a, has...preprocessing_pipeline: org.apache.spark.ml.Pipeline = pipeline_f5e5cde75c15\ntrainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [asin: string, overall: double ... 3 more fields]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [asin: string, overall: double ... 3 more fields]\nlabelColumn: String = label\nStarting Multinomial Logistic Regression\nlambda_par: Double = 0.02\nalpha_par: Double = 0.3\nen_mlr: org.apache.spark.ml.classification.LogisticRegression = logreg_386cc6dc2c4a\nmodel_stages: Array[org.apache.spark.ml.Estimator[_ >: org.apache.spark.ml.classification.LogisticRegressionModel with org.apache.spark.ml.PipelineModel <: org.apache.spark.ml.Model[_ >: org.apache.spark.ml.classification.LogisticRegressionModel with org.apache.spark.ml.PipelineModel <: org.apache.spark.ml.Transformer with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.util.MLWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.Estimator[_ >: org.apache.spark.ml.classification.LogisticRegressionModel with org.apache.spark.ml.PipelineModel <: org.apache.spark.ml.Transformer with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.util.MLWritable{def copy(extra: org.apache.spark.ml.param.ParamM...model_pipeline: org.apache.spark.ml.Pipeline = pipeline_5e09bbcf2d07\nparamGrid: Array[org.apache.spark.ml.param.ParamMap] =\nArray({\n\tlogreg_386cc6dc2c4a-elasticNetParam: 0.1,\n\tlogreg_386cc6dc2c4a-regParam: 0.01\n}, {\n\tlogreg_386cc6dc2c4a-elasticNetParam: 0.2,\n\tlogreg_386cc6dc2c4a-regParam: 0.01\n}, {\n\tlogreg_386cc6dc2c4a-elasticNetParam: 0.4,\n\tlogreg_386cc6dc2c4a-regParam: 0.01\n}, {\n\tlogreg_386cc6dc2c4a-elasticNetParam: 0.1,\n\tlogreg_386cc6dc2c4a-regParam: 0.05\n}, {\n\tlogreg_386cc6dc2c4a-elasticNetParam: 0.2,\n\tlogreg_386cc6dc2c4a-regParam: 0.05\n}, {\n\tlogreg_386cc6dc2c4a-elasticNetParam: 0.4,\n\tlogreg_386cc6dc2c4a-regParam: 0.05\n}, {\n\tlogreg_386cc6dc2c4a-elasticNetParam: 0.1,\n\tlogreg_386cc6dc2c4a-regParam: 0.1\n}, {\n\tlogreg_386cc6dc2c4a-elasticNetParam: 0.2,\n\tlogreg_386cc6dc2c4a-regParam: 0.1\n}, {\n\tlogreg_386cc6dc2c4a-elasticNetParam: 0.4,\n\tlogreg_386cc6dc2c4a-...evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_90f8c1da3c09\ncrossval: org.apache.spark.ml.tuning.CrossValidator = cv_d2afceaa8047\ncvModelMLR: org.apache.spark.ml.tuning.CrossValidatorModel = cv_d2afceaa8047\n<console>:3: error: ';' expected but 'val' found.\n%val cvModelMLR = PipelineModel.load(\"hdfs:///user/maria_dev/spark-mlr-cv-best\")\n ^\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer, StopWordsRemover, CountVectorizer, HashingTF, IDF}\n",
    "import scala.math.pow\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\n",
    "\n",
    "\n",
    "//// TEXT COLUMN PREPROCESSING\n",
    "//We define the RegexTokenizer() to convert reviews from plain text form to words\n",
    "val regextokenizer = new RegexTokenizer().setGaps(false)\n",
    "    .setPattern(\"\\\\w+(?:'\\\\w+)?|[^\\\\w\\\\s]\")\n",
    "    .setInputCol(\"reviewtext\")\n",
    "    .setOutputCol(\"reviewwords\")\n",
    "    \n",
    "    \n",
    "//We define remove the stop words from the reviewwords\n",
    "val sw_remover = new StopWordsRemover()\n",
    "    .setCaseSensitive(false)\n",
    "    .setInputCol(\"reviewwords\")\n",
    "    .setOutputCol(\"words_filtered\")\n",
    "    \n",
    "    \n",
    "//  Hashing TF to vectorize features\n",
    "val hashtf = new HashingTF()\n",
    "            .setInputCol(\"words_filtered\")\n",
    "            .setOutputCol(\"tf\")\n",
    "            .setNumFeatures(pow(2,12).toInt)\n",
    "            \n",
    "            \n",
    "// Lowers the terms of documents that are very common\n",
    "val idf = new IDF()\n",
    "        .setInputCol(\"tf\")\n",
    "        .setOutputCol(\"tfidf\")\n",
    "\n",
    "// CATEGORY COLUMN I.E. CATEGORICAL COLUMN PREPROCESSING\n",
    "// convert categorical column 'category' to labels; no rule for assigning the labels is used, the transformation is done in a random manner\n",
    "val indexer = new StringIndexer()\n",
    "        .setInputCol(\"category\")\n",
    "        .setOutputCol(\"categorylabel\")\n",
    "        .setHandleInvalid(\"skip\") //telling StringIndexer how to treat unseen labels - 'skip' will remove the rows entirely; 'keep' will assign the same index to all unseen\n",
    "\n",
    "\n",
    "// assemble two features columns into a 'features' column\n",
    "val vassembler = new VectorAssembler()\n",
    "        .setInputCols(Array(\"tfidf\", \"categorylabel\"))\n",
    "        .setOutputCol(\"features\")\n",
    "\n",
    "\n",
    "// Set the pipeline    \n",
    "val preprocessing_stages = Array(regextokenizer, sw_remover, hashtf, idf, indexer, vassembler)\n",
    "val preprocessing_pipeline = new Pipeline()\n",
    "                    .setStages(preprocessing_stages)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    \n",
    "// Split into training and test datasets and, just to be safe, define the target column                  \n",
    "val Array(trainingData, testData) = DF2.randomSplit(Array(0.8, 0.2), seed=42)\n",
    "val labelColumn = \"label\"\n",
    "\n",
    "\n",
    "println(\"Starting Multinomial Logistic Regression\")\n",
    "// Initialise hyperparameters that will be later tuned and define the multinomial logistic regression model, including lambda parameter corresponding to regularization\n",
    "val lambda_par = 0.02\n",
    "val alpha_par = 0.3\n",
    "val en_mlr = new LogisticRegression()\n",
    "        .setLabelCol(labelColumn)\n",
    "        .setFeaturesCol(\"features\")\n",
    "        .setRegParam(lambda_par)\n",
    "        .setMaxIter(100)\n",
    "        .setElasticNetParam(alpha_par)\n",
    "        .setFamily(\"multinomial\")\n",
    "        \n",
    "        \n",
    "\n",
    "// Define the full model pipeline - preprocessing + model       \n",
    "val model_stages = Array(preprocessing_pipeline, en_mlr) \n",
    "val model_pipeline = new Pipeline()\n",
    "                    .setStages(model_stages)\n",
    "                    \n",
    "                    \n",
    "// Define the param grid used for tuning the hyperparameters                    \n",
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(en_mlr.regParam, Array(0.01, 0.05, 0.1))\n",
    "  .addGrid(en_mlr.elasticNetParam, Array(0.1, 0.2, 0.4))\n",
    "  .build()\n",
    "  \n",
    "  \n",
    "// Define the evaluator used for evaluating the model\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "                .setLabelCol(labelColumn)\n",
    "\n",
    "\n",
    "// We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "// This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "// A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "// metricName() param for metric name in evaluation (supports \"f1\" (default), \"weightedPrecision\", \"weightedRecall\", \"accuracy\")\n",
    "// Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\n",
    "// is F1 score\n",
    "\n",
    "val crossval = new CrossValidator()\n",
    "  .setEstimator(model_pipeline)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(3)  // Use 3+ in practice\n",
    "  \n",
    "// Run cross-validation, and choose the best set of parameters.\n",
    "val cvModelMLR = crossval.fit(trainingData)\n",
    "\n",
    "// Now we can optionally save the fitted pipeline to disk\n",
    "// +cvModelMLR.write.overwrite().save(\"hdfs:///user/maria_dev/spark-mlr-cv-best\")\n",
    "\n",
    "// And load it back in during production\n",
    "val cvModelMLR = PipelineModel.load(\"hdfs:///user/maria_dev/spark-mlr-cv-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dominant-wildlife",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\nresults: org.apache.spark.sql.DataFrame = [asin: string, reviewtext: string ... 4 more fields]\npredictionAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[39528] at rdd at <console>:199\nmetrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@ce39930\nConfusion matrix:\n14125.0  2593.0  33628.0   \n3363.0   6536.0  46984.0   \n2692.0   3281.0  491108.0  \naccuracy: Double = 0.846865019609141\nSummary Statistics\nAccuracy = 0.846865019609141\nWeighted precision: 0.8144684381243038\nWeighted recall: 0.8468650196091411\nWeighted F1 score: 0.8070515918639329\nWeighted false positive rate: 0.6202997707641787\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "\n",
    "\n",
    "// Make predictions on test documents. cvModel uses the best model found (multinomial logistic regression model).\n",
    "val results = cvModelMLR.transform(testData)\n",
    "                     .select(\"asin\", \"reviewtext\", \"category\", \"label\", \"probability\", \"prediction\")\n",
    "                     \n",
    "// Compute raw scores on the test set\n",
    "val predictionAndLabels = results\n",
    "        .select($\"prediction\",$\"label\")\n",
    "        .as[(Double, Double)]\n",
    "        .rdd\n",
    "        \n",
    "// EVALUATE THE MODEL\n",
    "        \n",
    "// Instantiate metrics object\n",
    "val metrics = new MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "// Confusion matrix\n",
    "println(\"Confusion matrix:\")\n",
    "println(metrics.confusionMatrix)\n",
    "\n",
    "// Overall Statistics\n",
    "val accuracy = metrics.accuracy\n",
    "println(\"Summary Statistics\")\n",
    "println(s\"Accuracy = $accuracy\")\n",
    "\n",
    "\n",
    "// Weighted stats\n",
    "println(s\"Weighted precision: ${metrics.weightedPrecision}\")\n",
    "println(s\"Weighted recall: ${metrics.weightedRecall}\")\n",
    "println(s\"Weighted F1 score: ${metrics.weightedFMeasure}\")\n",
    "println(s\"Weighted false positive rate: ${metrics.weightedFalsePositiveRate}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
